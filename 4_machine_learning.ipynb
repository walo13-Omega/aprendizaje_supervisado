{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cce6c7db",
   "metadata": {},
   "source": [
    "\n",
    "## Comprendiendo GridSearch con un modelo individual\n",
    "\n",
    "Antes de ejecutar todos los modelos, veamos **cómo funciona GridSearch** con un solo modelo, la **Regresión Logística**.  \n",
    "Esto nos ayudará a entender qué hace exactamente el proceso y cómo elige los mejores hiperparámetros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19b8548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento simple de una Regresión Logística\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Crear y entrenar el modelo\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicción y evaluación\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Precisión del modelo: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72194ac6",
   "metadata": {},
   "source": [
    "\n",
    "### Ajustando Hiperparámetros Manualmente\n",
    "\n",
    "Si no especificamos hiperparámetros, Scikit Learn usa valores por defecto.  \n",
    "Podemos definirlos manualmente para ajustar el comportamiento del modelo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9a2b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresión Logística con hiperparámetros definidos manualmente\n",
    "\n",
    "model = LogisticRegression(\n",
    "    C=0.5,                  # Regularización\n",
    "    penalty='l2',           # Tipo de penalización\n",
    "    solver='lbfgs',         # Algoritmo de optimización\n",
    "    max_iter=200,           # Iteraciones máximas\n",
    "    class_weight='balanced' # Ajuste de clases\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicción y evaluación\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Precisión del modelo: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e427a08",
   "metadata": {},
   "source": [
    "\n",
    "###  ¿Qué hace GridSearch?\n",
    "\n",
    "GridSearch prueba **todas las combinaciones posibles de hiperparámetros** que especifiquemos.  \n",
    "Por ejemplo, para la Regresión Logística podemos definir el siguiente espacio de búsqueda:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a41da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parametros = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga'],\n",
    "    'max_iter': [100, 500, 1000]\n",
    "}\n",
    "\n",
    "# Crear el objeto GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=LogisticRegression(),\n",
    "    param_grid=parametros,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    verbose=0,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Ajustar el modelo\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar resultados\n",
    "y_pred = grid_search.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mejor combinación de hiperparámetros: {grid_search.best_params_}\")\n",
    "print(f\"Precisión del mejor modelo: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa086cbd",
   "metadata": {},
   "source": [
    "\n",
    " Con esto comprendemos cómo **GridSearch** busca la mejor combinación de hiperparámetros.  \n",
    "A continuación, aplicaremos el mismo principio a **todos los modelos** de clasificación en el conjunto Titanic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebe861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Modelos\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "\n",
    "# Métricas\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Guardado de modelos\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52de8ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset procesado\n",
    "df = pd.read_csv('./data/titanic_procesado.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c29817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar variables independientes (X) y dependiente (y)\n",
    "X = df.drop(['Survived'], axis=1)\n",
    "y = df['Survived']\n",
    "\n",
    "# División entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convertir a NumPy arrays\n",
    "X_train = X_train.values\n",
    "y_train = y_train.values\n",
    "X_test = X_test.values\n",
    "y_test = y_test.values\n",
    "\n",
    "print(\"Tamaños:\")\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"y_test:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc81c714",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelos = {\n",
    "    'Logistic Regression': {\n",
    "        'model': LogisticRegression(),\n",
    "        'params': {'model__C': [0.1], 'model__max_iter': [1000]}\n",
    "    },\n",
    "    'Support Vector Classifier': {\n",
    "        'model': SVC(),\n",
    "        'params': {'model__kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'model__C': [0.1, 1, 10]}\n",
    "    },\n",
    "    'Decision Tree Classifier': {\n",
    "        'model': DecisionTreeClassifier(),\n",
    "        'params': {'model__splitter': ['best', 'random'], 'model__max_depth': [None, 1, 2, 3, 4]}\n",
    "    },\n",
    "    'Random Forest Classifier': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {'model__n_estimators': [10, 100], 'model__max_depth': [None, 1, 2, 3, 4], 'model__max_features': ['auto', 'sqrt', 'log2']}\n",
    "    },\n",
    "    'Gradient Boosting Classifier': {\n",
    "        'model': GradientBoostingClassifier(),\n",
    "        'params': {'model__n_estimators': [10, 100], 'model__max_depth': [None, 1, 2, 3, 4]}\n",
    "    },\n",
    "    'AdaBoost Classifier': {\n",
    "        'model': AdaBoostClassifier(),\n",
    "        'params': {'model__n_estimators': [10, 100]}\n",
    "    },\n",
    "    'K-Nearest Neighbors Classifier': {\n",
    "        'model': KNeighborsClassifier(),\n",
    "        'params': {'model__n_neighbors': [3, 5, 7]}\n",
    "    },\n",
    "    'XGBoost Classifier': {\n",
    "        'model': XGBClassifier(),\n",
    "        'params': {'model__n_estimators': [10, 100], 'model__max_depth': [None, 1, 2, 3]}\n",
    "    },\n",
    "    'LGBM Classifier': {\n",
    "        'model': LGBMClassifier(),\n",
    "        'params': {'model__n_estimators': [10, 100], 'model__max_depth': [None, 1, 2, 3], 'model__learning_rate': [0.1, 0.2, 0.3], 'model__verbose': [-1]}\n",
    "    },\n",
    "    'GaussianNB': {\n",
    "        'model': GaussianNB(),\n",
    "        'params': {}\n",
    "    },\n",
    "    'Naive Bayes Classifier': {\n",
    "        'model': BernoulliNB(),\n",
    "        'params': {'model__alpha': [0.1, 1.0, 10.0]}\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d877a058",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = []\n",
    "\n",
    "for nombre, config in modelos.items():\n",
    "    print(f\"Entrenando {nombre}...\")\n",
    "    pipe = Pipeline([('model', config['model'])])\n",
    "    grid = GridSearchCV(pipe, config['params'], cv=3, scoring='accuracy', n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    best_model = grid.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    resultados.append((nombre, acc, grid.best_params_))\n",
    "\n",
    "# Mostrar resultados\n",
    "resultados_df = pd.DataFrame(resultados, columns=['Modelo', 'Precisión', 'Mejores Parámetros']).sort_values(by='Precisión', ascending=False)\n",
    "resultados_df.reset_index(drop=True, inplace=True)\n",
    "resultados_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19234131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el mejor modelo\n",
    "mejor_modelo = resultados_df.iloc[0]['Modelo']\n",
    "print(f\"Mejor modelo: {mejor_modelo}\")\n",
    "\n",
    "modelo_final = modelos[mejor_modelo]['model']\n",
    "modelo_final.fit(X_train, y_train)\n",
    "\n",
    "with open('./data/mejor_modelo.pkl', 'wb') as f:\n",
    "    pickle.dump(modelo_final, f)\n",
    "\n",
    "print(\"Modelo guardado como ./data/mejor_modelo.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9bc994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar el ajuste de los modelos con GridSearchCV\n",
    "\n",
    "# Definir los modelos y sus respectivos hiperparámetros para GridSearch\n",
    "modelos = {\n",
    "    'Regresión Logística': {\n",
    "        'modelo': LogisticRegression(),\n",
    "        'parametros': {\n",
    "            'C': [0.01, 0.1, 1, 10, 100],\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'solver': ['liblinear', 'saga'],\n",
    "            'max_iter': [100, 500, 1000]\n",
    "        }\n",
    "    },\n",
    "    'Clasificador de Vectores de Soporte': {\n",
    "        'modelo': SVC(),\n",
    "        'parametros': {\n",
    "            'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "            'C': [0.1, 1, 10]\n",
    "        }\n",
    "    },\n",
    "    'Clasificador de Árbol de Decisión': {\n",
    "        'modelo': DecisionTreeClassifier(),\n",
    "        'parametros': {\n",
    "            'splitter': ['best', 'random'],\n",
    "            'max_depth': [None, 1, 2, 3, 4]\n",
    "        }\n",
    "    },\n",
    "    'Clasificador de Bosques Aleatorios': {\n",
    "        'modelo': RandomForestClassifier(),\n",
    "        'parametros': {\n",
    "            'n_estimators': [10, 100],\n",
    "            'max_depth': [None, 1, 2, 3, 4],\n",
    "            'max_features': ['sqrt', 'log2', None]\n",
    "        }\n",
    "    },\n",
    "    'Clasificador de Gradient Boosting': {\n",
    "        'modelo': GradientBoostingClassifier(),\n",
    "        'parametros': {\n",
    "            'n_estimators': [10, 100],\n",
    "            'max_depth': [None, 1, 2, 3, 4]\n",
    "        }\n",
    "    },\n",
    "    'Clasificador AdaBoost': {\n",
    "        'modelo': AdaBoostClassifier(),\n",
    "        'parametros': {'n_estimators': [10, 100]}\n",
    "    },\n",
    "    'Clasificador K-Nearest Neighbors': {\n",
    "        'modelo': KNeighborsClassifier(),\n",
    "        'parametros': {'n_neighbors': [3, 5, 7]}\n",
    "    },\n",
    "    'Clasificador XGBoost': {\n",
    "        'modelo': XGBClassifier(),\n",
    "        'parametros': {'n_estimators': [10, 100], 'max_depth': [None, 1, 2, 3]}\n",
    "    },\n",
    "    'Clasificador LGBM': {\n",
    "        'modelo': LGBMClassifier(),\n",
    "        'parametros': {'n_estimators': [10, 100], 'max_depth': [None, 1, 2, 3], 'learning_rate': [0.1, 0.2, 0.3], 'verbose': [-1]}\n",
    "    },\n",
    "    'GaussianNB': {\n",
    "        'modelo': GaussianNB(),\n",
    "        'parametros': {}\n",
    "    },\n",
    "    'Clasificador Naive Bayes': {\n",
    "        'modelo': BernoulliNB(),\n",
    "        'parametros': {'alpha': [0.1, 1.0, 10.0]}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Inicializar variables para almacenar los puntajes de los modelos y el mejor estimador\n",
    "puntajes_modelos = []\n",
    "mejor_precision = 0\n",
    "mejor_estimador = None\n",
    "mejor_modelo = None\n",
    "estimadores = {}\n",
    "\n",
    "# Iterar sobre cada modelo y sus hiperparámetros\n",
    "for nombre, info_modelo in modelos.items():\n",
    "    print(f\"Entrenando {nombre}...\")\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=info_modelo['modelo'],\n",
    "        param_grid=info_modelo['parametros'],\n",
    "        cv=5,\n",
    "        scoring='accuracy',\n",
    "        verbose=0,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    precision = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    puntajes_modelos.append({'Modelo': nombre, 'Precisión': precision})\n",
    "    estimadores[nombre] = grid_search.best_estimator_\n",
    "\n",
    "    if precision > mejor_precision:\n",
    "        mejor_modelo = nombre\n",
    "        mejor_precision = precision\n",
    "        mejor_estimador = grid_search.best_estimator_\n",
    "\n",
    "# Convertir resultados a DataFrame y mostrar métricas\n",
    "metricas = pd.DataFrame(puntajes_modelos).sort_values('Precisión', ascending=False)\n",
    "\n",
    "print(\"Rendimiento de los modelos de clasificación\")\n",
    "print(metricas.round(2))\n",
    "print('---------------------------------------------------')\n",
    "print(\"MEJOR MODELO DE CLASIFICACIÓN\")\n",
    "print(f\"Modelo: {mejor_modelo}\")\n",
    "print(f\"Precisión: {mejor_precision:.2f}\")\n",
    "\n",
    "# Guardar el mejor modelo encontrado\n",
    "with open('./data/mejor_modelo_grid.pkl', 'wb') as f:\n",
    "    pickle.dump(mejor_estimador, f)\n",
    "\n",
    "print(\"✅ Mejor modelo guardado como ./data/mejor_modelo_grid.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f20e279",
   "metadata": {},
   "source": [
    "\n",
    "## Inferencia y Guardado del Modelo\n",
    "\n",
    "Llegamos al momento final: **usar nuestro modelo entrenado para predecir nuevos datos**.  \n",
    "Este proceso se llama *inferencia*, y consiste en aplicar el modelo sobre información nueva que **no vio durante el entrenamiento**.\n",
    "\n",
    "Podemos alimentar el modelo con datos de nuevos pasajeros y predecir si sobrevivirían o no.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1140c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos un ejemplo de dato del conjunto de entrenamiento\n",
    "print(\"Primer registro de X_train:\", X_train[0])\n",
    "print(\"Etiqueta correspondiente de y_train:\", y_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc09c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un nuevo array con los mismos valores de ejemplo\n",
    "nuevos_datos = np.array([0,1,0.6159084,0,0,0.55547282,1]).reshape(1,-1)\n",
    "\n",
    "# Realizar la predicción\n",
    "prediccion = mejor_estimador.predict(nuevos_datos)\n",
    "print(\"Predicción del modelo:\", prediccion)\n",
    "\n",
    "if prediccion[0] == 1:\n",
    "    print(\"✅ El modelo predice que el pasajero sobrevivió.\")\n",
    "else:\n",
    "    print(\"❌ El modelo predice que el pasajero no sobrevivió.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e40031",
   "metadata": {},
   "source": [
    "\n",
    "###  Guardar el Modelo con Pickle\n",
    "\n",
    "Para reutilizar nuestro modelo en producción (por ejemplo, dentro de una API o app web),  \n",
    "debemos guardarlo. Usaremos **Pickle**, que permite serializar objetos de Python en archivos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7c485d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "with open('modelo.pkl', 'wb') as archivo_estimador:\n",
    "    pickle.dump(mejor_estimador, archivo_estimador)\n",
    "\n",
    "print(\"✅ Modelo guardado correctamente como 'modelo.pkl'\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
